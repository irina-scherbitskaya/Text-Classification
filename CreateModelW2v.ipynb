{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import string\n",
    "import logging\n",
    "import warnings\n",
    "from gensim.models import Word2Vec\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159571\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('datasets/processed_train2.csv', index_col='id')\n",
    "test_X = pd.read_csv('datasets/processed_test2.csv', index_col='id')\n",
    "test_y = pd.read_csv('datasets/test_labels2.csv', index_col='id')\n",
    "types = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "X = train[['comment_text']]  \n",
    "y = train[types]\n",
    "\n",
    "new_X = [x.split() for x in X['comment_text']]\n",
    "new_X_test = [x.split() for x in test_X['comment_text']]\n",
    "\n",
    "print(len(new_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-20 21:32:58,523 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2019-04-20 21:32:58,524 : INFO : collecting all words and their counts\n",
      "2019-04-20 21:32:58,525 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-04-20 21:32:58,593 : INFO : PROGRESS: at sentence #10000, processed 244766 words, keeping 23659 word types\n",
      "2019-04-20 21:32:58,648 : INFO : PROGRESS: at sentence #20000, processed 487827 words, keeping 35548 word types\n",
      "2019-04-20 21:32:58,701 : INFO : PROGRESS: at sentence #30000, processed 724183 words, keeping 45307 word types\n",
      "2019-04-20 21:32:58,756 : INFO : PROGRESS: at sentence #40000, processed 971689 words, keeping 53872 word types\n",
      "2019-04-20 21:32:58,809 : INFO : PROGRESS: at sentence #50000, processed 1210530 words, keeping 61728 word types\n",
      "2019-04-20 21:32:58,864 : INFO : PROGRESS: at sentence #60000, processed 1462695 words, keeping 69087 word types\n",
      "2019-04-20 21:32:58,920 : INFO : PROGRESS: at sentence #70000, processed 1706650 words, keeping 76059 word types\n",
      "2019-04-20 21:32:58,975 : INFO : PROGRESS: at sentence #80000, processed 1950220 words, keeping 82435 word types\n",
      "2019-04-20 21:32:59,030 : INFO : PROGRESS: at sentence #90000, processed 2185228 words, keeping 88240 word types\n",
      "2019-04-20 21:32:59,084 : INFO : PROGRESS: at sentence #100000, processed 2426838 words, keeping 93974 word types\n",
      "2019-04-20 21:32:59,139 : INFO : PROGRESS: at sentence #110000, processed 2673825 words, keeping 99538 word types\n",
      "2019-04-20 21:32:59,193 : INFO : PROGRESS: at sentence #120000, processed 2908544 words, keeping 104940 word types\n",
      "2019-04-20 21:32:59,247 : INFO : PROGRESS: at sentence #130000, processed 3148485 words, keeping 110068 word types\n",
      "2019-04-20 21:32:59,304 : INFO : PROGRESS: at sentence #140000, processed 3388917 words, keeping 115320 word types\n",
      "2019-04-20 21:32:59,407 : INFO : PROGRESS: at sentence #150000, processed 3630962 words, keeping 120434 word types\n",
      "2019-04-20 21:32:59,480 : INFO : PROGRESS: at sentence #160000, processed 3873230 words, keeping 125494 word types\n",
      "2019-04-20 21:32:59,546 : INFO : PROGRESS: at sentence #170000, processed 4111556 words, keeping 132554 word types\n",
      "2019-04-20 21:32:59,612 : INFO : PROGRESS: at sentence #180000, processed 4345744 words, keeping 139126 word types\n",
      "2019-04-20 21:32:59,676 : INFO : PROGRESS: at sentence #190000, processed 4578342 words, keeping 145883 word types\n",
      "2019-04-20 21:32:59,740 : INFO : PROGRESS: at sentence #200000, processed 4814301 words, keeping 153154 word types\n",
      "2019-04-20 21:32:59,810 : INFO : PROGRESS: at sentence #210000, processed 5053967 words, keeping 159676 word types\n",
      "2019-04-20 21:32:59,901 : INFO : PROGRESS: at sentence #220000, processed 5291400 words, keeping 165970 word types\n",
      "2019-04-20 21:32:59,927 : INFO : collected 167909 word types from a corpus of 5373660 raw words and 223549 sentences\n",
      "2019-04-20 21:32:59,929 : INFO : Loading a fresh vocabulary\n",
      "2019-04-20 21:33:00,098 : INFO : effective_min_count=10 retains 21769 unique words (12% of original 167909, drops 146140)\n",
      "2019-04-20 21:33:00,099 : INFO : effective_min_count=10 leaves 5085385 word corpus (94% of original 5373660, drops 288275)\n",
      "2019-04-20 21:33:00,184 : INFO : deleting the raw counts dictionary of 167909 items\n",
      "2019-04-20 21:33:00,191 : INFO : sample=0.001 downsamples 22 most-common words\n",
      "2019-04-20 21:33:00,194 : INFO : downsampling leaves estimated 4883773 word corpus (96.0% of prior 5085385)\n",
      "2019-04-20 21:33:00,289 : INFO : estimated required memory for 21769 words and 30 dimensions: 16109060 bytes\n",
      "2019-04-20 21:33:00,289 : INFO : resetting layer weights\n",
      "2019-04-20 21:33:00,527 : INFO : training model with 2 workers on 21769 vocabulary and 30 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-04-20 21:33:01,533 : INFO : EPOCH 1 - PROGRESS: at 22.25% examples, 1097039 words/s, in_qsize 3, out_qsize 0\n",
      "2019-04-20 21:33:02,539 : INFO : EPOCH 1 - PROGRESS: at 44.78% examples, 1103118 words/s, in_qsize 3, out_qsize 0\n",
      "2019-04-20 21:33:03,542 : INFO : EPOCH 1 - PROGRESS: at 71.97% examples, 1178870 words/s, in_qsize 3, out_qsize 0\n",
      "2019-04-20 21:33:04,503 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-20 21:33:04,505 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-20 21:33:04,505 : INFO : EPOCH - 1 : training on 5373660 raw words (4884055 effective words) took 4.0s, 1228918 effective words/s\n",
      "2019-04-20 21:33:05,515 : INFO : EPOCH 2 - PROGRESS: at 26.48% examples, 1308801 words/s, in_qsize 3, out_qsize 0\n",
      "2019-04-20 21:33:06,518 : INFO : EPOCH 2 - PROGRESS: at 52.53% examples, 1292456 words/s, in_qsize 3, out_qsize 0\n",
      "2019-04-20 21:33:07,523 : INFO : EPOCH 2 - PROGRESS: at 78.51% examples, 1282093 words/s, in_qsize 3, out_qsize 0\n",
      "2019-04-20 21:33:08,272 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-20 21:33:08,279 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-20 21:33:08,280 : INFO : EPOCH - 2 : training on 5373660 raw words (4883485 effective words) took 3.8s, 1295198 effective words/s\n",
      "2019-04-20 21:33:09,290 : INFO : EPOCH 3 - PROGRESS: at 28.45% examples, 1410564 words/s, in_qsize 3, out_qsize 0\n",
      "2019-04-20 21:33:10,293 : INFO : EPOCH 3 - PROGRESS: at 53.08% examples, 1307657 words/s, in_qsize 3, out_qsize 0\n",
      "2019-04-20 21:33:11,298 : INFO : EPOCH 3 - PROGRESS: at 80.99% examples, 1321958 words/s, in_qsize 2, out_qsize 1\n",
      "2019-04-20 21:33:12,153 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-20 21:33:12,160 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-20 21:33:12,161 : INFO : EPOCH - 3 : training on 5373660 raw words (4883814 effective words) took 3.9s, 1260235 effective words/s\n",
      "2019-04-20 21:33:13,173 : INFO : EPOCH 4 - PROGRESS: at 25.76% examples, 1270358 words/s, in_qsize 3, out_qsize 0\n",
      "2019-04-20 21:33:14,174 : INFO : EPOCH 4 - PROGRESS: at 53.85% examples, 1324362 words/s, in_qsize 3, out_qsize 0\n",
      "2019-04-20 21:33:15,178 : INFO : EPOCH 4 - PROGRESS: at 82.73% examples, 1348541 words/s, in_qsize 3, out_qsize 0\n",
      "2019-04-20 21:33:15,741 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-20 21:33:15,748 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-20 21:33:15,749 : INFO : EPOCH - 4 : training on 5373660 raw words (4883678 effective words) took 3.6s, 1363036 effective words/s\n",
      "2019-04-20 21:33:16,755 : INFO : EPOCH 5 - PROGRESS: at 28.98% examples, 1442498 words/s, in_qsize 3, out_qsize 0\n",
      "2019-04-20 21:33:17,759 : INFO : EPOCH 5 - PROGRESS: at 51.01% examples, 1258893 words/s, in_qsize 3, out_qsize 0\n",
      "2019-04-20 21:33:18,764 : INFO : EPOCH 5 - PROGRESS: at 75.50% examples, 1235973 words/s, in_qsize 3, out_qsize 0\n",
      "2019-04-20 21:33:19,679 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-20 21:33:19,683 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-20 21:33:19,683 : INFO : EPOCH - 5 : training on 5373660 raw words (4883497 effective words) took 3.9s, 1243047 effective words/s\n",
      "2019-04-20 21:33:19,684 : INFO : training on a 26868300 raw words (24418529 effective words) took 19.2s, 1274752 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(new_X + new_X_test, min_count=10, size = 30, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-20 21:33:19,701 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2019-04-20 21:33:19,704 : INFO : training model with 2 workers on 21769 vocabulary and 30 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-04-20 21:33:20,713 : INFO : EPOCH 1 - PROGRESS: at 28.98% examples, 1437559 words/s, in_qsize 3, out_qsize 0\n",
      "2019-04-20 21:33:21,717 : INFO : EPOCH 1 - PROGRESS: at 58.85% examples, 1447674 words/s, in_qsize 3, out_qsize 0\n",
      "2019-04-20 21:33:22,717 : INFO : EPOCH 1 - PROGRESS: at 85.60% examples, 1395025 words/s, in_qsize 3, out_qsize 0\n",
      "2019-04-20 21:33:23,215 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-20 21:33:23,223 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-20 21:33:23,224 : INFO : EPOCH - 1 : training on 5373660 raw words (4884006 effective words) took 3.5s, 1389785 effective words/s\n",
      "2019-04-20 21:33:24,231 : INFO : EPOCH 2 - PROGRESS: at 28.98% examples, 1441857 words/s, in_qsize 3, out_qsize 0\n",
      "2019-04-20 21:33:25,237 : INFO : EPOCH 2 - PROGRESS: at 59.04% examples, 1452419 words/s, in_qsize 3, out_qsize 0\n",
      "2019-04-20 21:33:26,238 : INFO : EPOCH 2 - PROGRESS: at 89.54% examples, 1457321 words/s, in_qsize 3, out_qsize 0\n",
      "2019-04-20 21:33:26,582 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-20 21:33:26,583 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-20 21:33:26,584 : INFO : EPOCH - 2 : training on 5373660 raw words (4883963 effective words) took 3.4s, 1456342 effective words/s\n",
      "2019-04-20 21:33:27,592 : INFO : EPOCH 3 - PROGRESS: at 26.11% examples, 1293725 words/s, in_qsize 3, out_qsize 0\n",
      "2019-04-20 21:33:28,598 : INFO : EPOCH 3 - PROGRESS: at 54.22% examples, 1332519 words/s, in_qsize 3, out_qsize 0\n",
      "2019-04-20 21:33:29,606 : INFO : EPOCH 3 - PROGRESS: at 83.85% examples, 1364019 words/s, in_qsize 3, out_qsize 0\n",
      "2019-04-20 21:33:30,139 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-20 21:33:30,146 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-20 21:33:30,147 : INFO : EPOCH - 3 : training on 5373660 raw words (4884497 effective words) took 3.6s, 1372588 effective words/s\n",
      "2019-04-20 21:33:30,147 : INFO : training on a 16120980 raw words (14652466 effective words) took 10.4s, 1403177 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(14652466, 16120980)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(new_X + new_X_test, total_examples=len(new_X + new_X_test), epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-20 21:33:30,167 : INFO : saving Word2VecKeyedVectors object under train.model, separately None\n",
      "2019-04-20 21:33:30,169 : INFO : not storing attribute vectors_norm\n",
      "2019-04-20 21:33:30,172 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-04-20 21:33:30,261 : INFO : saved train.model\n"
     ]
    }
   ],
   "source": [
    "model.wv.save(\"train.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-20 21:33:30,270 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('fuckfac', 0.8223397731781006),\n",
       " ('kike', 0.7956886291503906),\n",
       " ('youi', 0.7926100492477417),\n",
       " ('fucktard', 0.7914835214614868),\n",
       " ('ashol', 0.7871485352516174),\n",
       " ('shiti', 0.7818084955215454),\n",
       " ('godamn', 0.7799387574195862),\n",
       " ('reali', 0.7750482559204102),\n",
       " ('retard', 0.772307276725769),\n",
       " ('fuckwit', 0.7686790823936462)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-20 21:33:30,303 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2019-04-20 21:33:30,307 : INFO : training model with 2 workers on 21769 vocabulary and 30 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-04-20 21:33:31,322 : INFO : EPOCH 1 - PROGRESS: at 35.60% examples, 1253252 words/s, in_qsize 3, out_qsize 0\n",
      "2019-04-20 21:33:32,328 : INFO : EPOCH 1 - PROGRESS: at 66.50% examples, 1168220 words/s, in_qsize 3, out_qsize 0\n",
      "2019-04-20 21:33:33,330 : INFO : EPOCH 1 - PROGRESS: at 99.77% examples, 1165178 words/s, in_qsize 1, out_qsize 1\n",
      "2019-04-20 21:33:33,331 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-20 21:33:33,334 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-20 21:33:33,335 : INFO : EPOCH - 1 : training on 3862220 raw words (3522125 effective words) took 3.0s, 1166274 effective words/s\n",
      "2019-04-20 21:33:33,336 : INFO : training on a 3862220 raw words (3522125 effective words) took 3.0s, 1163469 effective words/s\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.0249290e+00, -1.3534839e+00, -2.4453094e+00, -1.2859768e+00,\n",
       "       -3.0845299e-01, -1.5166361e+00, -1.3135763e+00,  1.0985616e+00,\n",
       "       -3.1627538e+00, -1.2794498e+00, -5.1816154e-01, -3.4759790e-01,\n",
       "       -3.4444842e-01, -3.3792245e-01, -2.2905932e-01,  8.3978462e-01,\n",
       "       -5.8684015e-01, -7.7062595e-01, -5.8401299e-01,  2.0685592e+00,\n",
       "       -3.0667827e-01,  1.7061046e-01, -1.2618657e-03, -1.2986947e+00,\n",
       "       -1.0021321e+00,  1.4636134e-01,  8.0514342e-02, -3.0350330e-01,\n",
       "        8.5345507e-01,  1.1291684e+00], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [21769, 159571]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-228abf32e55d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'toxic'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype,\n\u001b[0;32m-> 1216\u001b[0;31m                          order=\"C\")\n\u001b[0m\u001b[1;32m   1217\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 204\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [21769, 159571]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-20 21:33:33,736 : ERROR : Uncaught exception in ZMQStream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 421, in execute_request\n",
      "    self._abort_queues()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 636, in _abort_queues\n",
      "    self._abort_queue(stream)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 657, in _abort_queue\n",
      "    self._publish_status('idle', parent=msg)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 327, in _publish_status\n",
      "    ident=self._topic('status'),\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/jupyter_client/session.py\", line 713, in send\n",
      "    header=header, metadata=metadata)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/jupyter_client/session.py\", line 572, in msg\n",
      "    header = self.msg_header(msg_type) if header is None else header\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/jupyter_client/session.py\", line 562, in msg_header\n",
      "    return msg_header(self.msg_id, msg_type, self.username, self.session)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/jupyter_client/session.py\", line 512, in msg_id\n",
      "    return new_id()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/jupyter_client/session.py\", line 132, in new_id\n",
      "    return u'-'.join(b2a_hex(x).decode('ascii') for x in (\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
